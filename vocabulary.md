# Generative AI Vocabulary
*Author: Arulat Kami*  
*EPAM GenAI Fundamentals – Task #1*

---

This vocabulary contains key terms and concepts relevant for understanding and working with Generative AI systems. The definitions are written in my own words and reflect the terminology used in my research report. The glossary is intentionally concise and may be expanded throughout the course as new concepts appear.

---

## 1. Large Language Model (LLM)
A machine learning model trained on massive text datasets to predict the next token and generate coherent text. LLMs serve as the foundation for many GenAI applications, including chatbots and code generation tools.

## 2. Token
A small unit of text (a whole word, part of a word, or punctuation) that an LLM processes. Models do not work with words directly but operate on sequences of tokens.

## 3. Prompt
The instruction or input text given to a model to guide its output. Good prompt design helps the model understand the task and produce better results.

## 4. Hallucination
A situation when an AI model generates incorrect, made-up, or misleading information that looks confident but has no grounding in real data.

## 5. Business Value
The measurable benefit that a GenAI solution brings to an organization, such as reducing costs, accelerating processes, or improving user experience.

## 6. Technical Challenge
A limitation or difficulty that appears when designing or deploying an AI system, such as data quality issues, performance constraints, or integration complexity.

## 7. Weak Point
A known drawback or risk of an AI solution that may reduce its reliability or usability. Weak points must be considered when choosing or implementing AI tools.

## 8. Embedding
A numerical vector representation of text used to compare meaning and perform relevant search in RAG systems. Similar texts have similar embeddings.

## 9. Vector Database
A specialized storage system that holds embeddings and allows fast similarity search. Commonly used in RAG pipelines for retrieving relevant documents.

## 10. Retrieval-Augmented Generation (RAG)
An architecture that combines document retrieval with text generation. The model fetches relevant context first and then generates grounded answers based on real data.

## 11. Multimodal Model
A model that can understand and generate not only text but also images, audio, or other data types. Multimodality enhances flexibility and expands use cases.

## 12. Auto-Completion
A feature where AI predicts the next fragment of code or text to speed up writing. Widely used in coding assistants such as Copilot or Codeium.

## 13. Text-to-Image Model
A model that generates images from textual prompts. These models help designers prototype ideas quickly.

## 14. ASR (Automatic Speech Recognition)
Technology that converts spoken language into text. Used in transcription tools, voice assistants, and call center automation.

## 15. TTS (Text-to-Speech)
AI systems that transform written text into synthetic speech. Modern TTS models generate natural and expressive voices.

## 16. Context Window
The maximum amount of text (in tokens) that a model can “see” at once. A larger context window allows the model to reason over longer documents.

## 17. Fine-Tuning
A method of training an existing model on domain-specific data to improve performance in a narrow area. Often used to adapt general LLMs to enterprise tasks.

## 18. Code Generation
The process where a model produces source code based on prompts or instructions. Widely used to automate repetitive programming tasks.

## 19. Latency
The time it takes for an AI model to generate a response. Latency significantly affects user experience and system performance, especially in real-time applications.

## 20. Copyright Restriction
A legal limitation where generated images or text may unintentionally include or resemble copyrighted material. This creates risk when using AI-generated content commercially.

---

## Notes
This vocabulary may be expanded during the course as new concepts, tools, and patterns appear.
